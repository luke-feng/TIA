{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b9ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeb03255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "from lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import copy\n",
    "from attack_models_sc4 import _edge_group_clustering, GATEncoder, Decoder, GAE, threshold_by_otsu, contrastive_normalization_np_withpower, build_sparse_edge_otsu, build_sparse_edge\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "from tia_util import remove_diag_reshape, reconstruct_with_diagonal, contrastive_normalization_np, reconstruct\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cfe12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "350cf10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "metrics_root = f\"{cur_dir}\\\\saved_metrics\\\\\"\n",
    "topology_path = f\"{cur_dir}/topologies/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91527ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPOLOGY = [\"star\", \"ring\",\"ER_0.3\", \"ER_0.5\", \"ER_0.7\",'Abilene', 'brain', 'GÃ‰ANT', 'synth50', 'rf1755', 'rf3967', 'atlanta', 'brain',  'cost266', 'dfn-bwin', 'dfn-gwin', 'di-yuan', 'france',  'germany50', 'giul39', 'india35', 'janos-us', 'janos-us-ca', 'newyork', 'nobel-eu', 'nobel-germany', 'nobel-us', 'norway', 'pdh', 'pioro40', 'polska', 'sun', 'ta1', 'ta2', 'zib54']\n",
    "NUM_CLIENTS = [10, 20,30,12,22,50,79,87, 15, 161, 37, 11, 25, 35, 26, 39, 16, 28, 17, 14,  40,  27, 24, 65, 54]\n",
    "TOPOLOGY = [\"star\", \"ring\",\"ER_0.3\", \"ER_0.5\", \"ER_0.7\"]\n",
    "# NUM_CLIENTS = [10, 20,30]\n",
    "# DATASET = [\"Cifar10no\", \"Cifar10\", \"Mnist\",\"FMnist\", \"imagenet10\", \"pcam\", \"svhn\"]\n",
    "DATASET = [\"Cifar10\"]\n",
    "MODEL = [\"mlp\", \"mobile\",\"resnet\",\"pf\"]\n",
    "# MODEL = [\"mlp\", \"mobile\", \"resnet\"]\n",
    "IID = [1]\n",
    "MAX_EPOCHS = [3]\n",
    "ALPHA = 0.1\n",
    "SEED = [42]\n",
    "all_metrics_sc = os.listdir(metrics_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "289cc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_sc = os.listdir(metrics_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "387a69b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': 0.7098666666666666, 'alg': 'GNN', 'dataset': 'Cifar10', 'topo': 'star', 'iid': 1, 'max_epoch': 3, 'num': 10, 'density': 0.2, 'approach': '_loss_metric'}\n",
      "{'value': 0.7721385781911104, 'alg': 'GNN', 'dataset': 'Cifar10', 'topo': 'star', 'iid': 1, 'max_epoch': 3, 'num': 20, 'density': 0.1, 'approach': '_loss_metric'}\n",
      "{'value': 0.8302356409584639, 'alg': 'GNN', 'dataset': 'Cifar10', 'topo': 'star', 'iid': 1, 'max_epoch': 3, 'num': 30, 'density': 0.06666666666666667, 'approach': '_loss_metric'}\n",
      "{'value': 0.9900905046786318, 'alg': 'GNN', 'dataset': 'Cifar10', 'topo': 'ring', 'iid': 1, 'max_epoch': 3, 'num': 10, 'density': 0.2222222222222222, 'approach': '_loss_metric'}\n",
      "{'value': 0.9950540118214551, 'alg': 'GNN', 'dataset': 'Cifar10', 'topo': 'ring', 'iid': 1, 'max_epoch': 3, 'num': 20, 'density': 0.10526315789473684, 'approach': '_loss_metric'}\n",
      "{'value': 0.9903219072272557, 'alg': 'GNN', 'dataset': 'Cifar10', 'topo': 'ring', 'iid': 1, 'max_epoch': 3, 'num': 30, 'density': 0.06896551724137931, 'approach': '_loss_metric'}\n",
      "{'value': 0.9497856563549993, 'alg': 'GNN', 'dataset': 'Cifar10', 'topo': 'ER_0.3', 'iid': 1, 'max_epoch': 3, 'num': 10, 'density': 0.35555555555555557, 'approach': '_loss_metric'}\n",
      "{'value': 0.855, 'alg': 'GNN', 'dataset': 'Cifar10', 'topo': 'ER_0.3', 'iid': 1, 'max_epoch': 3, 'num': 20, 'density': 0.2631578947368421, 'approach': '_loss_metric'}\n",
      "{'value': 0.7983466974097781, 'alg': 'GNN', 'dataset': 'Cifar10', 'topo': 'ER_0.3', 'iid': 1, 'max_epoch': 3, 'num': 30, 'density': 0.26436781609195403, 'approach': '_loss_metric'}\n",
      "{'value': 0.7394652406417113, 'alg': 'GNN', 'dataset': 'Cifar10', 'topo': 'ER_0.5', 'iid': 1, 'max_epoch': 3, 'num': 10, 'density': 0.5111111111111111, 'approach': '_loss_metric'}\n",
      "{'value': 0.7077567751104391, 'alg': 'GNN', 'dataset': 'Cifar10', 'topo': 'ER_0.5', 'iid': 1, 'max_epoch': 3, 'num': 20, 'density': 0.45789473684210524, 'approach': '_loss_metric'}\n",
      "{'value': 0.6578440838969671, 'alg': 'GNN', 'dataset': 'Cifar10', 'topo': 'ER_0.5', 'iid': 1, 'max_epoch': 3, 'num': 30, 'density': 0.46436781609195404, 'approach': '_loss_metric'}\n",
      "{'value': 0.8285513784461154, 'alg': 'GNN', 'dataset': 'Cifar10', 'topo': 'ER_0.7', 'iid': 1, 'max_epoch': 3, 'num': 10, 'density': 0.6222222222222222, 'approach': '_loss_metric'}\n",
      "{'value': 0.5200241545893719, 'alg': 'GNN', 'dataset': 'Cifar10', 'topo': 'ER_0.7', 'iid': 1, 'max_epoch': 3, 'num': 20, 'density': 0.6947368421052632, 'approach': '_loss_metric'}\n",
      "{'value': 0.4429656443774091, 'alg': 'GNN', 'dataset': 'Cifar10', 'topo': 'ER_0.7', 'iid': 1, 'max_epoch': 3, 'num': 30, 'density': 0.6850574712643678, 'approach': '_loss_metric'}\n"
     ]
    }
   ],
   "source": [
    "all_res = []\n",
    "for dataset in DATASET:\n",
    "        for model_name in MODEL:\n",
    "            # dataset and model setting\n",
    "            for topo in TOPOLOGY:\n",
    "               for iid in IID:\n",
    "                    for seed in SEED:\n",
    "                        for max_epoch in MAX_EPOCHS:\n",
    "                            for num in NUM_CLIENTS:\n",
    "                                ROUND =  num\n",
    "                                toponame_file = f\"{topology_path}/{num}_{topo}.pk\"\n",
    "                                if os.path.exists(toponame_file):  \n",
    "                                    with open(toponame_file, \"rb\") as f:\n",
    "                                        G = pk.load(f)\n",
    "                                        density = nx.density(G)\n",
    "                                        label = nx.adjacency_matrix(G).todense()\n",
    "                                else:\n",
    "                                    continue\n",
    "                                \n",
    "                                scenario_name = dataset + \"_\" + model_name + \"_\" + topo + \"_\" + str(iid) + \"_\" + str(ALPHA) + \"_\" + str(seed) + \"_\" + str(max_epoch) + \"_\" + str(num)\n",
    "                                if scenario_name in all_metrics_sc:\n",
    "                                    all_metrics = {}\n",
    "                                    # for approach in ['_cosine_metric', '_curvature_divergence', '_entropy_metric', '_euclidean_metric', '_jacobian_metric', '_loss_metric']:\n",
    "                                    try:\n",
    "                                        for approach in ['_cosine_metric', '_loss_metric']:                                                                          \n",
    "                                            metric_path = os.path.join(metrics_root,scenario_name, f'{approach}.csv')                                        \n",
    "                                            metrics = pd.read_csv(metric_path, index_col=0)\n",
    "                                            all_metrics[approach] = metrics\n",
    "                                    except:\n",
    "                                        continue\n",
    "                                            \n",
    "                                    value_ls1 = copy.deepcopy(all_metrics['_cosine_metric'])\n",
    "                                    reshaped1 = contrastive_normalization_np_withpower(value_ls1, eta=3)\n",
    "                                    value_ls2 = copy.deepcopy(all_metrics['_loss_metric'])\n",
    "                                    reshaped2 = contrastive_normalization_np_withpower(value_ls2, eta=3)\n",
    "\n",
    "                                    reshaped = (reshaped1 + (1-reshaped2))/2\n",
    "                                    # reshaped = reshaped1\n",
    "                                    \n",
    "                                    # value_ls = copy.deepcopy(all_metrics[approach])\n",
    "                                    # reshaped = contrastive_normalization_np_withpower(value_ls, eta=3)\n",
    "                                    np.fill_diagonal(reshaped, 0)\n",
    "                                    \n",
    "                                    # for clustering_name in [\"Kmeans\", \"GMM\", \"Spectral\"]:             \n",
    "                                    #     infer_con,_ = _edge_group_clustering(reshaped, clustering_name, flag=True, multi_dimensional=False)\n",
    "                                    #     infer_adj = reconstruct(infer_con)\n",
    "                                    #     f1 = f1_score(label.flatten(), infer_adj.flatten(), average='weighted')\n",
    "                                    #     res = {}\n",
    "                                    #     res['value'] = f1\n",
    "                                    #     res['alg'] = clustering_name\n",
    "                                    #     res['dataset'] = dataset\n",
    "                                    #     res['topo'] = topo                                        \n",
    "                                    #     res['iid'] = iid\n",
    "                                    #     res['max_epoch'] = max_epoch\n",
    "                                    #     res['num'] = num\n",
    "                                    #     res['density'] = density\n",
    "                                    #     res['approach'] = approach\n",
    "                                    #     all_res.append(res)\n",
    "                                        \n",
    "                                    sim_matrix_np = np.copy(reshaped)\n",
    "                                    sim_matrix_np = contrastive_normalization_np_withpower(sim_matrix_np, eta=4)\n",
    "                                    np.fill_diagonal(sim_matrix_np, 1)\n",
    "                                    sim_matrix = torch.tensor(sim_matrix_np, dtype=torch.float32)\n",
    "                                    x = sim_matrix\n",
    "                                    edge_index, edge_weight = build_sparse_edge(sim_matrix)\n",
    "                                    # edge_index, edge_weight, sparse_th = build_sparse_edge(sim_matrix)\n",
    "                                    data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight)\n",
    "                                    encoder = GATEncoder(in_channels=x.size(1), hidden_channels=32, out_channels=16)\n",
    "                                    decoder = Decoder(input_dim=16)\n",
    "                                    model = GAE(encoder, decoder)\n",
    "                                    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "                                    for epoch in range(200):\n",
    "                                        model.train()\n",
    "                                        optimizer.zero_grad()\n",
    "                                        adj_pred = model(data.x, data.edge_index)\n",
    "                                        loss = F.mse_loss(adj_pred, sim_matrix)\n",
    "                                        loss.backward()\n",
    "                                        optimizer.step()\n",
    "                                        \n",
    "                                    model.eval()\n",
    "                                    with torch.no_grad():\n",
    "                                        adj_pred = model(data.x, data.edge_index)\n",
    "                                        # edge_matrix, threshold = threshold_by_knee(adj_pred)\n",
    "                                        # adj_pred = soft_threshold(adj_pred, tau=0.7)\n",
    "                                        edge_matrix, threshold = threshold_by_otsu(adj_pred)\n",
    "                                    edge_matrix.fill_diagonal_(0)\n",
    "                                    f1 = f1_score(label.flatten(), edge_matrix.flatten(), average='weighted')\n",
    "                                    res = {}\n",
    "                                    res['value'] = f1\n",
    "                                    res['alg'] = 'GNN'\n",
    "                                    res['dataset'] = dataset\n",
    "                                    res['topo'] = topo                                        \n",
    "                                    res['iid'] = iid\n",
    "                                    res['max_epoch'] = max_epoch\n",
    "                                    res['num'] = num\n",
    "                                    res['density'] = density\n",
    "                                    res['approach'] = approach\n",
    "                                    all_res.append(res)\n",
    "                                    print(res)\n",
    "\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "580f98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(all_res)\n",
    "res_df.to_csv('sc3_cfaug.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
