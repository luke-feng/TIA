{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset.mnist import MNISTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from Model.mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Python39_64\\\\python.exe/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMNISTDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m train_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mtrain_set))\n\u001b[0;32m      4\u001b[0m data_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2500\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\luke-\\Documents\\git\\TIA\\Dataset\\mnist.py:23\u001b[0m, in \u001b[0;36mMNISTDataset.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Singletons of MNIST train and test datasets\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msys\u001b[38;5;241m.\u001b[39mpath[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/data\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m MNISTDataset\u001b[38;5;241m.\u001b[39mmnist_train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     MNISTDataset\u001b[38;5;241m.\u001b[39mmnist_train \u001b[38;5;241m=\u001b[39m MNIST(\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msys\u001b[38;5;241m.\u001b[39mpath[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/data\u001b[39m\u001b[38;5;124m\"\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mCompose([\n\u001b[0;32m     28\u001b[0m                                                                 transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     29\u001b[0m                                                                 transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, ), (\u001b[38;5;241m0.5\u001b[39m, ))\n\u001b[0;32m     30\u001b[0m                                                             ])\n\u001b[0;32m     31\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\lib\\os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Python39_64\\\\python.exe/data'"
     ]
    }
   ],
   "source": [
    "dataset = MNISTDataset()\n",
    "\n",
    "train_indices = np.arange(len(dataset.train_set))\n",
    "data_size=2500\n",
    "num_client=10\n",
    "train_subsets_indices = [train_indices[i * data_size:(i + 1) * data_size] for i in range(num_client)]\n",
    "train_subsets = [Subset(dataset.train_set, indices) for indices in train_subsets_indices]\n",
    " \n",
    "for i in range(1):   \n",
    "    train_loader = DataLoader(train_subsets[i], batch_size=128, shuffle=True, num_workers=0)\n",
    "    model = MLP()\n",
    "    local_trainer = Trainer(max_epochs=10, accelerator=\"auto\", devices=\"auto\", logger=False,\n",
    "                                                            # callbacks = [MyCustomCheckpoint(save_dir=f\"{model_directory}/Local_models/Round_{r}\",\n",
    "                                                            # idx=client.idx, rou=r, logger=model_logger)],\n",
    "                                                            enable_checkpointing=False, enable_model_summary=False, enable_progress_bar=True)\n",
    "                                            \n",
    "    local_trainer.fit(model, train_loader)                                       \n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset.svhn import SVHNdataset\n",
    "from Model.svhnresnet18 import SVHNResNet18\n",
    "dataset = SVHNdataset()\n",
    "\n",
    "train_indices = np.arange(len(dataset.train_set))\n",
    "data_size=2500\n",
    "num_client=10\n",
    "train_subsets_indices = [train_indices[i * data_size:(i + 1) * data_size] for i in range(num_client)]\n",
    "train_subsets = [Subset(dataset.train_set, indices) for indices in train_subsets_indices]\n",
    " \n",
    "for i in range(1):   \n",
    "    train_loader = DataLoader(train_subsets[i], batch_size=128, shuffle=True, num_workers=0)\n",
    "    model = SVHNResNet18()\n",
    "    local_trainer = Trainer(max_epochs=10, accelerator=\"auto\", devices=\"auto\", logger=False,\n",
    "                                                            # callbacks = [MyCustomCheckpoint(save_dir=f\"{model_directory}/Local_models/Round_{r}\",\n",
    "                                                            # idx=client.idx, rou=r, logger=model_logger)],\n",
    "                                                            enable_checkpointing=False, enable_model_summary=False, enable_progress_bar=True)\n",
    "                                            \n",
    "    local_trainer.fit(model, train_loader)    \n",
    "test_indices = np.arange(len(dataset.test_set))\n",
    "data_size=2500\n",
    "num_client=10\n",
    "test_subsets_indices = [test_indices[i * data_size:(i + 1) * data_size] for i in range(num_client)]\n",
    "test_subsets = [Subset(dataset.test_set, indices) for indices in test_subsets_indices]\n",
    "test_loader = DataLoader(test_subsets[0], batch_size=128, shuffle=True, num_workers=0)\n",
    "local_trainer.test(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
