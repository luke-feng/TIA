{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import networkx as nx\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_object(num, name):\n",
    "    if name == \"fully\":\n",
    "        G = nx.complete_graph(num)\n",
    "    elif name == \"star\":\n",
    "        G = nx.star_graph(num - 1)\n",
    "    elif name == \"ring\":\n",
    "        G = nx.cycle_graph(num)\n",
    "    elif name.startswith(\"ER_\"):\n",
    "        try:\n",
    "            # Extract probability and seed from the graph_info\n",
    "            parts = name.split(\"_\")\n",
    "            prob = float(parts[1])\n",
    "            G = nx.erdos_renyi_graph(num, prob, seed=45)\n",
    "        except (IndexError, ValueError):\n",
    "            raise ValueError(\"Invalid format for ER graph type. Expected 'ER_prob'\")\n",
    "    else:\n",
    "        raise ValueError(\"Not supported topology.\")        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in [10,20,30]:\n",
    "    for name in [\"fully\", \"star\", \"ring\", \"ER_0.3\", \"ER_0.7\", \"ER_0.5\"]:\n",
    "        G = create_graph_object(num, name)\n",
    "        with open(f'{num}_{name}.pk','wb') as f:\n",
    "            pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abilene_edges.csv 12\n",
      "GÉANT_edges.csv 22\n",
      "synth50_edges.csv 50\n",
      "rf1755_edges.csv 87\n",
      "rf3967_edges.csv 79\n"
     ]
    }
   ],
   "source": [
    "for file_name in ['Abilene_edges.csv', 'GÉANT_edges.csv', 'synth50_edges.csv', 'rf1755_edges.csv', 'rf3967_edges.csv']:\n",
    "    edges_df = pandas.read_csv(file_name)\n",
    "    parts = file_name.split(\"_\")\n",
    "    topo = parts[0]\n",
    "    G = nx.DiGraph()\n",
    "    for _, row in edges_df.iterrows():\n",
    "        G.add_edge(row['src'], row['dst'])\n",
    "    adj_matrix = nx.adjacency_matrix(G).todense()\n",
    "    print(file_name, len(adj_matrix))\n",
    "    with open(f'{len(adj_matrix)}_{topo}.pk','wb') as f:\n",
    "        pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abilene 12\n",
      "atlanta 15\n",
      "brain 161\n",
      "cost266 37\n",
      "dfn-bwin 10\n",
      "dfn-gwin 11\n",
      "di-yuan 11\n",
      "france 25\n",
      "geant 22\n",
      "germany50 50\n",
      "giul39 39\n",
      "india35 35\n",
      "janos-us 26\n",
      "janos-us-ca 39\n",
      "newyork 16\n",
      "nobel-eu 28\n",
      "nobel-germany 17\n",
      "nobel-us 14\n",
      "norway 27\n",
      "pdh 11\n",
      "pioro40 40\n",
      "polska 12\n",
      "sun 27\n",
      "ta1 24\n",
      "ta2 65\n",
      "zib54 54\n",
      "['abilene', 'atlanta', 'brain', 'cost266', 'dfn-bwin', 'dfn-gwin', 'di-yuan', 'france', 'geant', 'germany50', 'giul39', 'india35', 'janos-us', 'janos-us-ca', 'newyork', 'nobel-eu', 'nobel-germany', 'nobel-us', 'norway', 'pdh', 'pioro40', 'polska', 'sun', 'ta1', 'ta2', 'zib54']\n",
      "[12, 15, 161, 37, 10, 11, 11, 25, 22, 50, 39, 35, 26, 39, 16, 28, 17, 14, 27, 11, 40, 12, 27, 24, 65, 54]\n"
     ]
    }
   ],
   "source": [
    "NETWORK_LIST = [\n",
    "    \"abilene\", \"atlanta\", \"brain\", \"cost266\", \"dfn-bwin\", \"dfn-gwin\", \"di-yuan\",\n",
    "    \"france\", \"geant\", \"germany50\", \"giul39\", \"india35\", \"janos-us\", \"janos-us-ca\",\n",
    "    \"newyork\", \"nobel-eu\", \"nobel-germany\", \"nobel-us\", \"norway\", \"pdh\", \"pioro40\",\n",
    "    \"polska\", \"sun\", \"ta1\", \"ta2\", \"zib54\"\n",
    "]\n",
    "\n",
    "BASE_URL = \"https://sndlib.put.poznan.pl/download/sndlib-networks-xml/\"\n",
    "\n",
    "def find_tag_ignore_namespace(root, tag):\n",
    "    return next((elem for elem in root.iter() if elem.tag.endswith(tag)), None)\n",
    "\n",
    "def find_all_tags_ignore_namespace(root, tag):\n",
    "    return [elem for elem in root.iter() if elem.tag.endswith(tag)]\n",
    "\n",
    "def safe_find_text(elem, tag):\n",
    "    for child in elem:\n",
    "        if child.tag.endswith(tag):\n",
    "            return child.text\n",
    "    return None\n",
    "\n",
    "def parse_and_save_adj_matrix(name, save_dir=\"adj_matrices\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    url = f\"{BASE_URL}{name}.xml\"\n",
    "    # print(f\"Fetching {url}\")\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "        root = ET.fromstring(r.content)\n",
    "\n",
    "        G = nx.DiGraph(name=name)\n",
    "\n",
    "        node_elements = find_all_tags_ignore_namespace(root, 'node')\n",
    "        link_elements = find_all_tags_ignore_namespace(root, 'link')\n",
    "\n",
    "        if not node_elements or not link_elements:\n",
    "            raise ValueError(\"Missing <node> or <link> in XML\")\n",
    "\n",
    "        # Map node IDs to 0-based indices\n",
    "        node_id_map = {}\n",
    "        # print(len(node_elements))\n",
    "        # print(len(link_elements))\n",
    "        for idx, node in enumerate(node_elements):\n",
    "            node_id_map[node.attrib['id']] = idx\n",
    "            G.add_node(idx)\n",
    "\n",
    "        # Add edges using 0-based node indices\n",
    "        for link in link_elements:\n",
    "            src = safe_find_text(link, 'source')\n",
    "            dst = safe_find_text(link, 'target')\n",
    "            if src is not None and dst is not None:\n",
    "                G.add_edge(node_id_map[src], node_id_map[dst])\n",
    "        # Save adjacency matrix\n",
    "        adj_matrix = nx.adjacency_matrix(G).todense()\n",
    "        \n",
    "        print(name, len(adj_matrix))\n",
    "        # with open(f'{len(adj_matrix)}_{name}.pk','wb') as f:\n",
    "        #     pickle.dump(G, f)\n",
    "        return len(adj_matrix)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {name}: {e}\")\n",
    "\n",
    "def process_all_networks():\n",
    "    nodes = []\n",
    "    topo = []\n",
    "    for name in NETWORK_LIST:\n",
    "        topo.append(name)\n",
    "        num = parse_and_save_adj_matrix(name)\n",
    "        nodes.append(num)\n",
    "        time.sleep(0.5)  # polite delay\n",
    "    print(topo)\n",
    "    print(nodes)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_networks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topofiles = os.listdir()\n",
    "infos = []\n",
    "for topof in all_topofiles:\n",
    "    if '.pk' in topof:\n",
    "        with open(topof, \"rb\") as f:\n",
    "            G = pk.load(f)\n",
    "            num_nodes = G.number_of_nodes()\n",
    "            num_edges = G.number_of_edges()\n",
    "            avg_degree = sum(dict(G.degree()).values()) / num_nodes\n",
    "            density = nx.density(G)\n",
    "            g_info = [topof, num_nodes, num_edges,avg_degree ,density]\n",
    "            infos.append(g_info)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(infos, columns=['topo', 'num_nodes', 'num_edges','avg_degree' ,'density'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('topo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
